{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "  Text Corpus   \n",
      "                                     Unigram Probability\tBigram probability \n",
      "<s> He read a book </s>              0.00058\t\t0.00095\n",
      "<s> I read a different book </s>     0.00003\t\t0.00006\n",
      "<s> He read a book my Mulan </s>     0.00000\t\t0.00002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "beginsentence = \"<s>\" #sentences are beginning with <s>\n",
    "endsentence = \"</s>\" #sentences are ending with </s>\n",
    "UNK = None\n",
    "\n",
    "class Unigram:\n",
    "    def __init__(self, sentences, smoothing=False):\n",
    "        self.unigramfrequency = dict()\n",
    "        self.lengthofcorpus = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                self.unigramfrequency[word] = self.unigramfrequency.get(word, 0) + 1\n",
    "                if word != beginsentence and word != endsentence:\n",
    "                    self.lengthofcorpus += 1 \n",
    "        self.uniquewords = len(self.unigramfrequency) - 2 #subtract 2 because unigramfrequency dictionary contains values for beginsentence and endsentence\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def unigramprobability(self, word):\n",
    "            numerator = self.unigramfrequency.get(word, 0)\n",
    "            denominator = self.lengthofcorpus\n",
    "            if self.smoothing:\n",
    "                numerator += 1 #add one more to total number of seen unique words for UNK - unseen events\n",
    "                denominator += self.uniquewords + 1\n",
    "            return float(numerator) / float(denominator)\n",
    "        \n",
    "    def unigramsentenceprobability(self, sentence, normalizeprobability=True):\n",
    "        logsum = 0\n",
    "        for word in sentence:\n",
    "            if word != beginsentence and word != endsentence:\n",
    "                wordprobability = self.unigramprobability(word)\n",
    "                logsum += math.log(wordprobability, 2)\n",
    "        return math.pow(2, logsum) if normalizeprobability else logsum          \n",
    "  \n",
    "    def vocabulary(self):\n",
    "        vocab = list(self.unigramfrequency.keys())\n",
    "        vocab.remove(beginsentence)\n",
    "        vocab.remove(endsentence)\n",
    "        vocab.sort()\n",
    "        vocab.append(UNK)\n",
    "        vocab.append(beginsentence)\n",
    "        vocab.append(endsentence)\n",
    "        return vocab\n",
    "    \n",
    "class Bigram(Unigram):\n",
    "    def __init__(self, sentences, smoothing=False):\n",
    "        Unigram.__init__(self, sentences, smoothing)\n",
    "        self.bigramfrequency = dict()\n",
    "        self.uniquebigram = set()\n",
    "        for sentence in sentences:\n",
    "            previousword = None\n",
    "            for word in sentence:\n",
    "                if previousword != None:\n",
    "                    self.bigramfrequency[(previousword, word)] = self.bigramfrequency.get((previousword, word),0) + 1\n",
    "                    if previousword != beginsentence and word != endsentence:\n",
    "                        self.uniquebigram.add((previousword, word))\n",
    "                previousword = word #we subtracted two for the Unigram model as the unigramfrequency dictionary  \n",
    "        self.uniquebigramwords = len(self.unigramfrequency) #contains values for beginsentence and endsentence but these need to be included in Bigram\n",
    "\n",
    "    def bigramprobabilty(self, previousword, word):\n",
    "        numerator = self.bigramfrequency.get((previousword, word), 0)\n",
    "        denominator = self.unigramfrequency.get(previousword, 0)\n",
    "        if self.smoothing:\n",
    "            numerator += 1\n",
    "            denominator += self.uniquebigramwords\n",
    "        return 0.0 if numerator == 0 or denominator == 0 else float(\n",
    "            numerator) / float(denominator)\n",
    "\n",
    "    def bigramsentenceprobability(self, sentence, normalizeprobability=True):\n",
    "        logsum = 0\n",
    "        previousword = None\n",
    "        for word in sentence:\n",
    "            if previousword != None:\n",
    "                bigramwordprobability = self.bigramprobabilty(previousword, word)\n",
    "                logsum += math.log(bigramwordprobability, 2)\n",
    "            previousword = word\n",
    "        return math.pow(2,\n",
    "                        logsum) if normalizeprobability else logsum\n",
    "\n",
    "    def vocab(self):\n",
    "        vocab = list(self.unigramfrequency.keys())\n",
    "        vocab.remove(beginsentence)\n",
    "        vocab.remove(endsentence)\n",
    "        vocab.sort()\n",
    "        vocab.append(UNK)\n",
    "        vocab.append(beginsentence)\n",
    "        vocab.append(endsentence)\n",
    "        return vocab\n",
    "\n",
    "def displayunigramprob(sorted_vocab_keys, model):\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != beginsentence and vocab_key != endsentence:\n",
    "            print(\"{}: {}\".format(vocab_key if vocab_key != UNK else \"UNK\", model.unigramprobability(vocab_key)), end=\" \")\n",
    "    \n",
    "def displaybigramprob(sorted_vocab_keys, model):\n",
    "    print(\"\\t\\t\", end=\"\")\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != beginsentence:\n",
    "            print(vocab_key if vocab_key != UNK else \"UNK\", end=\"\\t\\t\")\n",
    "    print(\"\")\n",
    "    for vocab_key in sorted_vocab_keys:\n",
    "        if vocab_key != beginsentence:\n",
    "            print(vocab_key if vocab_key != UNK else \"UNK\", end=\"\\t\\t\")\n",
    "            for vocab_key_second in sorted_vocab_keys:\n",
    "                if vocab_key_second != beginsentence:\n",
    "                    print(\"{0:.5f}\".format(model.bigramprobabilty(vocab_key, vocab_key_second)), end=\"\\t\\t\")\n",
    "            print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "def readfile(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [re.split(\"\\s+\", line.rstrip('\\n')) for line in f]\n",
    "\n",
    "text = readfile(\"C:/Users/user/OneDrive/YEAR 3/1ST SEMESTER/TXSA/TXSA_Assignment/Group Assignment Data/Text Corpus.txt\")    \n",
    "text2 = readfile(\"C:/Users/user/OneDrive/YEAR 3/1ST SEMESTER/TXSA/TXSA_Assignment/Group Assignment Data/Text Corpus.txt\")\n",
    "\n",
    "unsmoothed = Bigram(text)\n",
    "smoothed = Bigram(text, smoothing=True)\n",
    "sorted_vocab_keys = unsmoothed.vocab()\n",
    "\n",
    "print(\" \\n  Text Corpus   \")\n",
    "longest_sentence_len = max([len(\" \".join(sentence)) for sentence in text2]) + 5\n",
    "print(\" \", \" \" * (longest_sentence_len - len(\" \") - 2), \"Unigram Probability\\tBigram probability \")\n",
    "for sentence in text2:\n",
    "    sentence_string = \" \".join(sentence)\n",
    "    print(sentence_string, end=\" \" * (longest_sentence_len - len(sentence_string)))\n",
    "    print(\"{0:.5f}\".format(smoothed.unigramsentenceprobability(sentence)), end=\"\\t\\t\")\n",
    "    print(\"{0:.5f}\".format(smoothed.bigramsentenceprobability(sentence)))        \n",
    "        \n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
